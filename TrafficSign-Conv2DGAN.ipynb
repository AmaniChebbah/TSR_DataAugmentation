{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TEDS-Conv2DGAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kY5SlcPumq8N","colab_type":"code","colab":{}},"cell_type":"code","source":["#### GOOGLE DRIVE SPECIFIC ##########################\n","# Make sure that you have GPU selected in the Runtime\n","# If you do, will print Found GPU at: /device:GPU:0\n","# Else go to Runtime -> Change Runtime Type\n","\n","import tensorflow as tf\n","# device_name = tf.test.gpu_device_name()\n","# if device_name != '/device:GPU:0':\n","#   raise SystemError('GPU device not found')\n","# print('Found GPU at: {}'.format(device_name))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0T-TJQGSyfs3","colab_type":"code","outputId":"5425c8da-60ce-4483-9c49-ac750cf29960","executionInfo":{"status":"ok","timestamp":1552867386358,"user_tz":240,"elapsed":3917,"user":{"displayName":"Abdurrahman Cam","photoUrl":"https://lh3.googleusercontent.com/-k26CU-cLkKc/AAAAAAAAAAI/AAAAAAAAADA/qhdRTerZnrE/s64/photo.jpg","userId":"12952516305974938564"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# CODE SNIPPET TO ACCESS THE FILES IN GOOGLE DRIVE (GO TO BROWSER AND VERIFY)\n","# THEN YOU CAN ACCESS THE FILES ON LEFT SIDEBAR (copy path)\n","# (https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q#scrollTo=H4SJ-tGNkOeY)\n","\n","# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","# drive.mount('/content/drive')\n","drive.mount(\"/content/drive\", force_remount=True)\n","\n","# After executing the cell above, Drive\n","# files will be present in \"/content/drive/My Drive\".\n","# !ls \"/content/drive/My Drive\"\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"metadata":{"id":"tEgacIZfy5B7","colab_type":"code","colab":{}},"cell_type":"code","source":["# googlepath exists for googledrive finding the files. Can be set to empty \n","# string if running on computer. Have companylist.csv in same dir. Will create\n","# a stock_data folder and download stocks data into that. Currently just AAPL.\n","googlepath = \"drive/My Drive/SeniorDesign19/GANAttempts\"\n","foldernamepath = googlepath + \"/Data/32BY32\"\n","savedpath = googlepath + \"/TEDS_Presentation/savedImages/\"\n","modelpath = googlepath + \"/TEDS_Presentation/savedModels/\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"jAcBAE9H5q5h","colab_type":"code","outputId":"a0d88fee-51e1-4dab-fe46-82905fb02d0b","executionInfo":{"status":"ok","timestamp":1552867386375,"user_tz":240,"elapsed":3870,"user":{"displayName":"Abdurrahman Cam","photoUrl":"https://lh3.googleusercontent.com/-k26CU-cLkKc/AAAAAAAAAAI/AAAAAAAAADA/qhdRTerZnrE/s64/photo.jpg","userId":"12952516305974938564"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["from __future__ import print_function, division\n","\n","from keras.layers import Input, Dense, Flatten, Dropout, Reshape\n","from keras.layers import BatchNormalization, Activation, Conv2D, Conv2DTranspose\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.models import Model, load_model\n","from keras.optimizers import Adam\n","\n","from keras.datasets import cifar10\n","import keras.backend as K\n","\n","from keras.models import model_from_json\n","\n","\n","import matplotlib.pyplot as plt\n","\n","import sys\n","import numpy as np\n","import cv2\n","import os\n","import re\n","\n","import numpy as np\n","\n","import random\n","\n","%pylab inline"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Populating the interactive namespace from numpy and matplotlib\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['random']\n","`%matplotlib` prevents importing * from pylab and numpy\n","  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"],"name":"stderr"}]},{"metadata":{"id":"IBL0xYV56btS","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_generator(input_layer):\n","  '''\n","  Requires the input layer as input, outputs the model and the final layer\n","  '''\n","  \n","  hid = Dense(128 * 16 * 16, activation='relu')(input_layer)    \n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","  hid = Reshape((16, 16, 128))(hid)\n","\n","  hid = Conv2D(128, kernel_size=5, strides=1,padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)    \n","  #hid = Dropout(0.5)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Conv2DTranspose(128, 4, strides=2, padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  #hid = Dropout(0.5)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Conv2D(128, kernel_size=5, strides=1, padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","                      \n","  hid = Conv2D(3, kernel_size=5, strides=1, padding=\"same\")(hid)\n","  out = Activation(\"tanh\")(hid)\n","\n","  model = Model(input_layer, out)\n","  model.summary()\n","  \n","  return model, out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KTTBDCMYKos8","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_discriminator(input_layer):\n","  '''\n","  Requires the input layer as input, outputs the model and the final layer\n","  '''\n","\n","  hid = Conv2D(128, kernel_size=3, strides=1, padding='same')(input_layer)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Conv2D(128, kernel_size=4, strides=2, padding='same')(hid)\n","  hid = BatchNormalization(momentum=0.9)(hid)\n","  hid = LeakyReLU(alpha=0.1)(hid)\n","\n","  hid = Flatten()(hid)\n","  hid = Dropout(0.4)(hid)\n","  out = Dense(1, activation='sigmoid')(hid)\n","\n","  model = Model(input_layer, out)\n","\n","  model.summary()\n","\n","  return model, out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rEaPkXRgKsE_","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.preprocessing import image\n","\n","def generate_noise(n_samples, noise_dim):\n","  X = np.random.normal(0, 1, size=(n_samples, noise_dim))\n","  return X\n","\n","def show_imgs(batchidx):\n","  noise = generate_noise(9, 100)\n","  gen_imgs = generator.predict(noise)\n","\n","  fig, axs = plt.subplots(3, 3)\n","  count = 0\n","  for i in range(3):\n","    for j in range(3):\n","      # Dont scale the images back, let keras handle it\n","      img = image.array_to_img(gen_imgs[count], scale=True)\n","      axs[i,j].imshow(img)\n","      axs[i,j].axis('off')\n","      count += 1\n","#   plt.show()\n","  fig.savefig(savedpath + \"{}.png\".format(epoch + starting_epoch))\n","  plt.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UbyGmQoxKujm","colab_type":"code","outputId":"3f22224e-df5b-4049-9b42-935fc6140178","executionInfo":{"status":"ok","timestamp":1552867393876,"user_tz":240,"elapsed":11287,"user":{"displayName":"Abdurrahman Cam","photoUrl":"https://lh3.googleusercontent.com/-k26CU-cLkKc/AAAAAAAAAAI/AAAAAAAAADA/qhdRTerZnrE/s64/photo.jpg","userId":"12952516305974938564"}},"colab":{"base_uri":"https://localhost:8080/","height":1703}},"cell_type":"code","source":["# GAN creation\n","starting_epoch = 0\n","\n","model_names = os.listdir(modelpath)\n","model_names.sort(key = lambda x : (int(x.split('_')[0]), x.split('_')[1][:2]))\n","print(model_names)\n","\n","if len(model_names) != 0:\n","    # disc, gan, gen \n","    one = model_names[-3]\n","    m = re.search(r'[a-z]+', one)\n","    assert m.group() == \"disc\", print(\"wanted: disc, got: \", m.group())\n","    \n","    two = model_names[-2]\n","    m = re.search(r'[a-z]+', two)\n","    assert m.group() == \"gan\", print(\"wanted: gan, got: \", m.group())\n","    \n","    three = model_names[-1]\n","    m = re.search(r'[a-z]+', three)\n","    assert m.group() == \"gen\", print(\"wanted: gen, got: \", m.group())\n","    \n","    img_input = Input(shape=(32,32,3))\n","    discriminator, disc_out = get_discriminator(img_input)\n","    discriminator.load_weights(modelpath + one)\n","    discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    discriminator.trainable = False\n","\n","    noise_input = Input(shape=(100,))\n","    generator, gen_out = get_generator(noise_input)\n","    generator.load_weights(modelpath + three)\n","\n","    gan_input = Input(shape=(100,))    \n","    x = generator(gan_input)\n","    gan_out = discriminator(x)\n","    gan = Model(gan_input, gan_out)\n","    gan.summary()\n","\n","    gan.load_weights(modelpath + two)\n","    gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n","    \n","    m = re.search(r'([0-9]+)?', one)\n","    starting_epoch = int(m.group())\n","else:\n","    img_input = Input(shape=(32,32,3))\n","    discriminator, disc_out = get_discriminator(img_input)\n","    discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    discriminator.trainable = False\n","\n","    noise_input = Input(shape=(100,))\n","    generator, gen_out = get_generator(noise_input)\n","\n","    gan_input = Input(shape=(100,))    \n","    x = generator(gan_input)\n","    gan_out = discriminator(x)\n","    gan = Model(gan_input, gan_out)\n","    gan.summary()\n","\n","    gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['0_disc.h5', '0_gan.hdf5', '0_gen.h5', '5_disc.h5', '5_gan.hdf5', '5_gen.h5', '10_disc.h5', '10_gan.hdf5', '10_gen.h5', '15_disc.h5', '15_gan.hdf5', '15_gen.h5', '20_disc.h5', '20_gan.hdf5', '20_gen.h5', '25_disc.h5', '25_gan.hdf5', '25_gen.h5', '30_disc.h5', '30_gan.hdf5', '30_gen.h5', '35_disc.h5', '35_gan.hdf5', '35_gen.h5', '40_disc.h5', '40_gan.hdf5', '40_gen.h5', '45_disc.h5', '45_gan.hdf5', '45_gen.h5', '50_disc.h5', '50_gan.hdf5', '50_gen.h5', '55_disc.h5', '55_gan.hdf5', '55_gen.h5', '60_disc.h5', '60_gan.hdf5', '60_gen.h5', '65_disc.h5', '65_gan.hdf5', '65_gen.h5', '70_disc.h5', '70_gan.hdf5', '70_gen.h5', '75_disc.h5', '75_gan.hdf5', '75_gen.h5', '80_disc.h5', '80_gan.hdf5', '80_gen.h5', '85_disc.h5', '85_gan.hdf5', '85_gen.h5', '90_disc.h5', '90_gan.hdf5', '90_gen.h5', '95_disc.h5', '95_gan.hdf5', '95_gen.h5', '100_disc.h5', '100_gan.hdf5', '100_gen.h5', '105_disc.h5', '105_gan.hdf5', '105_gen.h5', '110_disc.h5', '110_gan.hdf5', '110_gen.h5', '115_disc.h5', '115_gan.hdf5', '115_gen.h5', '120_disc.h5', '120_gan.hdf5', '120_gen.h5', '125_disc.h5', '125_gan.hdf5', '125_gen.h5', '130_disc.h5', '130_gan.hdf5', '130_gen.h5', '135_disc.h5', '135_gan.hdf5', '135_gen.h5', '140_disc.h5', '140_gan.hdf5', '140_gen.h5', '145_disc.h5', '145_gan.hdf5', '145_gen.h5', '150_disc.h5', '150_gan.hdf5', '150_gen.h5', '155_disc.h5', '155_gan.hdf5', '155_gen.h5', '160_disc.h5', '160_gan.hdf5', '160_gen.h5', '165_disc.h5', '165_gan.hdf5', '165_gen.h5', '170_disc.h5', '170_gan.hdf5', '170_gen.h5', '175_disc.h5', '175_gan.hdf5', '175_gen.h5', '180_disc.h5', '180_gan.hdf5', '180_gen.h5', '185_disc.h5', '185_gan.hdf5', '185_gen.h5', '190_disc.h5', '190_gan.hdf5', '190_gen.h5', '195_disc.h5', '195_gan.hdf5', '195_gen.h5', '200_disc.h5', '200_gan.hdf5', '200_gen.h5', '205_disc.h5', '205_gan.hdf5', '205_gen.h5', '210_disc.h5', '210_gan.hdf5', '210_gen.h5', '215_disc.h5', '215_gan.hdf5', '215_gen.h5', '220_disc.h5', '220_gan.hdf5', '220_gen.h5', '225_disc.h5', '225_gan.hdf5', '225_gen.h5', '230_disc.h5', '230_gan.hdf5', '230_gen.h5', '235_disc.h5', '235_gan.hdf5', '235_gen.h5', '240_disc.h5', '240_gan.hdf5', '240_gen.h5', '245_disc.h5', '245_gan.hdf5', '245_gen.h5']\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 32, 32, 3)         0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 32, 32, 128)       3584      \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)   (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 16, 16, 128)       262272    \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 16, 16, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)   (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 8, 8, 128)         262272    \n","_________________________________________________________________\n","batch_normalization_12 (Batc (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_12 (Conv2D)           (None, 4, 4, 128)         262272    \n","_________________________________________________________________\n","batch_normalization_13 (Batc (None, 4, 4, 128)         512       \n","_________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)   (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 2048)              0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 2048)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 2049      \n","=================================================================\n","Total params: 794,497\n","Trainable params: 793,473\n","Non-trainable params: 1,024\n","_________________________________________________________________\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         (None, 100)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 32768)             3309568   \n","_________________________________________________________________\n","batch_normalization_14 (Batc (None, 32768)             131072    \n","_________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)   (None, 32768)             0         \n","_________________________________________________________________\n","reshape_2 (Reshape)          (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv2d_13 (Conv2D)           (None, 16, 16, 128)       409728    \n","_________________________________________________________________\n","batch_normalization_15 (Batc (None, 16, 16, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       262272    \n","_________________________________________________________________\n","batch_normalization_16 (Batc (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)   (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_14 (Conv2D)           (None, 32, 32, 128)       409728    \n","_________________________________________________________________\n","batch_normalization_17 (Batc (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)   (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_15 (Conv2D)           (None, 32, 32, 128)       409728    \n","_________________________________________________________________\n","batch_normalization_18 (Batc (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_18 (LeakyReLU)   (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_16 (Conv2D)           (None, 32, 32, 3)         9603      \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 32, 32, 3)         0         \n","=================================================================\n","Total params: 4,943,747\n","Trainable params: 4,877,187\n","Non-trainable params: 66,560\n","_________________________________________________________________\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         (None, 100)               0         \n","_________________________________________________________________\n","model_5 (Model)              (None, 32, 32, 3)         4943747   \n","_________________________________________________________________\n","model_4 (Model)              (None, 1)                 794497    \n","=================================================================\n","Total params: 5,738,244\n","Trainable params: 4,877,187\n","Non-trainable params: 861,057\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"YltMiIuQKylv","colab_type":"code","outputId":"68d1569e-5f78-4947-df6c-8043d9653e58","executionInfo":{"status":"ok","timestamp":1552867412763,"user_tz":240,"elapsed":30156,"user":{"displayName":"Abdurrahman Cam","photoUrl":"https://lh3.googleusercontent.com/-k26CU-cLkKc/AAAAAAAAAAI/AAAAAAAAADA/qhdRTerZnrE/s64/photo.jpg","userId":"12952516305974938564"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["BATCH_SIZE = 50\n","\n","# # Get training images\n","# (X_train, y_train), (X_test, _) = cifar10.load_data()\n","\n","## MODIFIDED TO TAKE IMAGES FROM FOLDER########\n","img_names = os.listdir(foldernamepath)\n","random.shuffle(img_names)\n","x = []\n","for img_name in img_names:\n","    if img_name is not None:\n","        im = np.asarray(cv2.imread(foldernamepath + \"/\" + img_name, cv2.IMREAD_COLOR))\n","        if im.shape[0] == 32 and im.shape[1] == 32:\n","            x.append(im)\n","\n","X_train = np.asarray(x)  \n","\n","# Select Cars\n","# X_train = X_train[y_train[:,0]==1]\n","print (\"Training shape: {}\".format(X_train.shape))\n","\n","# Normalize data\n","X_train = (X_train - 127.5) / 127.5\n"," \n","num_batches = int(X_train.shape[0]/BATCH_SIZE)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training shape: (4170, 32, 32, 3)\n"],"name":"stdout"}]},{"metadata":{"id":"jF6mvjQUK0ZT","colab_type":"code","outputId":"61877e7b-0fe9-4658-a46b-e9253e9e15d8","executionInfo":{"status":"ok","timestamp":1552879502222,"user_tz":240,"elapsed":3195425,"user":{"displayName":"Abdurrahman Cam","photoUrl":"https://lh3.googleusercontent.com/-k26CU-cLkKc/AAAAAAAAAAI/AAAAAAAAADA/qhdRTerZnrE/s64/photo.jpg","userId":"12952516305974938564"}},"colab":{"base_uri":"https://localhost:8080/","height":8551}},"cell_type":"code","source":["N_EPOCHS = 500\n","for epoch in range(N_EPOCHS):\n","\n","  cum_d_loss = 0.\n","  cum_g_loss = 0.\n","  \n","  for batch_idx in range(num_batches):\n","    # Get the next set of real images to be used in this iteration\n","    images = X_train[batch_idx*BATCH_SIZE : (batch_idx+1)*BATCH_SIZE]\n","\n","    noise_data = generate_noise(BATCH_SIZE, 100)\n","    generated_images = generator.predict(noise_data)\n","\n","    # Train on soft labels (add noise to labels as well)\n","    noise_prop = 0.05 # Randomly flip 5% of labels\n","    \n","    # Prepare labels for real data\n","    true_labels = np.zeros((BATCH_SIZE, 1)) + np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n","    flipped_idx = np.random.choice(np.arange(len(true_labels)), size=int(noise_prop*len(true_labels)))\n","    true_labels[flipped_idx] = 1 - true_labels[flipped_idx]\n","    \n","    # Train discriminator on real data\n","    d_loss_true = discriminator.train_on_batch(images, true_labels)\n","\n","    # Prepare labels for generated data\n","    gene_labels = np.ones((BATCH_SIZE, 1)) - np.random.uniform(low=0.0, high=0.1, size=(BATCH_SIZE, 1))\n","    flipped_idx = np.random.choice(np.arange(len(gene_labels)), size=int(noise_prop*len(gene_labels)))\n","    gene_labels[flipped_idx] = 1 - gene_labels[flipped_idx]\n","    \n","    # Train discriminator on generated data\n","    d_loss_gene = discriminator.train_on_batch(generated_images, gene_labels)\n","\n","    d_loss = 0.5 * np.add(d_loss_true, d_loss_gene)\n","    cum_d_loss += d_loss\n","\n","    # Train generator\n","    noise_data = generate_noise(BATCH_SIZE, 100)\n","    g_loss = gan.train_on_batch(noise_data, np.zeros((BATCH_SIZE, 1)))\n","    cum_g_loss += g_loss\n","\n","  print('  Epoch: {}, Generator Loss: {}, Discriminator Loss: {}'.format(epoch+1+starting_epoch, cum_g_loss/num_batches, cum_d_loss/num_batches))\n","  \n","  show_imgs(\"epoch\" + str(epoch + starting_epoch))\n","  \n","  if epoch % 5 == 0:\n","    generator.save_weights(modelpath + \"{}_gen.h5\".format(epoch + starting_epoch))\n","    discriminator.save_weights(modelpath + \"{}_disc.h5\".format(epoch + starting_epoch))\n","    gan.save_weights(modelpath + \"{}_gan.hdf5\".format(epoch + starting_epoch))\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["  Epoch: 246, Generator Loss: 2.608170440398067, Discriminator Loss: [0.33188346 0.        ]\n","  Epoch: 247, Generator Loss: 2.5154886260090104, Discriminator Loss: [0.3225231 0.       ]\n","  Epoch: 248, Generator Loss: 2.5960344866097693, Discriminator Loss: [0.3250629 0.       ]\n","  Epoch: 249, Generator Loss: 2.5932011144707, Discriminator Loss: [0.3230726 0.       ]\n","  Epoch: 250, Generator Loss: 2.570059639861785, Discriminator Loss: [0.32142484 0.        ]\n","  Epoch: 251, Generator Loss: 2.682399097695408, Discriminator Loss: [0.32577348 0.        ]\n","  Epoch: 252, Generator Loss: 2.600976103759674, Discriminator Loss: [0.32483602 0.        ]\n","  Epoch: 253, Generator Loss: 2.6353662028370133, Discriminator Loss: [0.3180913 0.       ]\n","  Epoch: 254, Generator Loss: 2.691952577556472, Discriminator Loss: [0.32541198 0.        ]\n","  Epoch: 255, Generator Loss: 2.662971980600472, Discriminator Loss: [0.32146972 0.        ]\n","  Epoch: 256, Generator Loss: 2.653186723410365, Discriminator Loss: [0.32340512 0.        ]\n","  Epoch: 257, Generator Loss: 2.605481364640845, Discriminator Loss: [0.3249025 0.       ]\n","  Epoch: 258, Generator Loss: 2.6328791624092194, Discriminator Loss: [0.32199684 0.        ]\n","  Epoch: 259, Generator Loss: 2.6231704591268517, Discriminator Loss: [0.32033563 0.        ]\n","  Epoch: 260, Generator Loss: 2.5802650193133987, Discriminator Loss: [0.31904033 0.        ]\n","  Epoch: 261, Generator Loss: 2.6172011832156814, Discriminator Loss: [0.3219457 0.       ]\n","  Epoch: 262, Generator Loss: 2.624546369874334, Discriminator Loss: [0.32253468 0.        ]\n","  Epoch: 263, Generator Loss: 2.6666388856359275, Discriminator Loss: [0.32039928 0.        ]\n","  Epoch: 264, Generator Loss: 2.66360794779766, Discriminator Loss: [0.32286978 0.        ]\n","  Epoch: 265, Generator Loss: 2.628568689507174, Discriminator Loss: [0.3181717 0.       ]\n","  Epoch: 266, Generator Loss: 2.6928452712943756, Discriminator Loss: [0.3182299 0.       ]\n","  Epoch: 267, Generator Loss: 2.6052421862820543, Discriminator Loss: [0.3244281 0.       ]\n","  Epoch: 268, Generator Loss: 2.620275944112295, Discriminator Loss: [0.31721273 0.        ]\n","  Epoch: 269, Generator Loss: 2.6162277675536743, Discriminator Loss: [0.319975 0.      ]\n","  Epoch: 270, Generator Loss: 2.5946398427687494, Discriminator Loss: [0.32281178 0.        ]\n","  Epoch: 271, Generator Loss: 2.6337401076971765, Discriminator Loss: [0.31766284 0.        ]\n","  Epoch: 272, Generator Loss: 2.748660810022469, Discriminator Loss: [0.31917477 0.        ]\n","  Epoch: 273, Generator Loss: 2.6191524554447954, Discriminator Loss: [0.31973168 0.        ]\n","  Epoch: 274, Generator Loss: 2.655367314097393, Discriminator Loss: [0.3213697 0.       ]\n","  Epoch: 275, Generator Loss: 2.6843782462269425, Discriminator Loss: [0.3211095 0.       ]\n","  Epoch: 276, Generator Loss: 2.610061529171036, Discriminator Loss: [0.31605637 0.        ]\n","  Epoch: 277, Generator Loss: 2.6422074889562217, Discriminator Loss: [0.32041073 0.        ]\n","  Epoch: 278, Generator Loss: 2.590589271970542, Discriminator Loss: [0.31873658 0.        ]\n","  Epoch: 279, Generator Loss: 2.5573999364692046, Discriminator Loss: [0.3194421 0.       ]\n","  Epoch: 280, Generator Loss: 2.607396266546594, Discriminator Loss: [0.31264362 0.        ]\n","  Epoch: 281, Generator Loss: 2.7039289819188865, Discriminator Loss: [0.31541467 0.        ]\n","  Epoch: 282, Generator Loss: 2.618500073272062, Discriminator Loss: [0.31757128 0.        ]\n","  Epoch: 283, Generator Loss: 2.5945717067603606, Discriminator Loss: [0.31817552 0.        ]\n","  Epoch: 284, Generator Loss: 2.5995835238192453, Discriminator Loss: [0.3158361 0.       ]\n","  Epoch: 285, Generator Loss: 2.538432968668191, Discriminator Loss: [0.31732363 0.        ]\n","  Epoch: 286, Generator Loss: 2.554147338292685, Discriminator Loss: [0.32083553 0.        ]\n","  Epoch: 287, Generator Loss: 2.6138712900230683, Discriminator Loss: [0.31904206 0.        ]\n","  Epoch: 288, Generator Loss: 2.6080898635358696, Discriminator Loss: [0.31875536 0.        ]\n","  Epoch: 289, Generator Loss: 2.6456923772053544, Discriminator Loss: [0.32164297 0.        ]\n","  Epoch: 290, Generator Loss: 2.6911179645951973, Discriminator Loss: [0.31860307 0.        ]\n","  Epoch: 291, Generator Loss: 2.655135354363775, Discriminator Loss: [0.3214814 0.       ]\n","  Epoch: 292, Generator Loss: 2.6313078561461114, Discriminator Loss: [0.31641093 0.        ]\n","  Epoch: 293, Generator Loss: 2.6796478147966316, Discriminator Loss: [0.31899083 0.        ]\n","  Epoch: 294, Generator Loss: 2.652905342090561, Discriminator Loss: [0.31647795 0.        ]\n","  Epoch: 295, Generator Loss: 2.6316417412585524, Discriminator Loss: [0.3169987 0.       ]\n","  Epoch: 296, Generator Loss: 2.6812717354441262, Discriminator Loss: [0.31932265 0.        ]\n","  Epoch: 297, Generator Loss: 2.6599261660173714, Discriminator Loss: [0.3157074 0.       ]\n","  Epoch: 298, Generator Loss: 2.6481659828898416, Discriminator Loss: [0.31686774 0.        ]\n","  Epoch: 299, Generator Loss: 2.631577463035124, Discriminator Loss: [0.3149441 0.       ]\n","  Epoch: 300, Generator Loss: 2.579425138163279, Discriminator Loss: [0.31912306 0.        ]\n","  Epoch: 301, Generator Loss: 2.623506761458983, Discriminator Loss: [0.31921664 0.        ]\n","  Epoch: 302, Generator Loss: 2.6493544880166113, Discriminator Loss: [0.31463498 0.        ]\n","  Epoch: 303, Generator Loss: 2.562303699642779, Discriminator Loss: [0.3149724 0.       ]\n","  Epoch: 304, Generator Loss: 2.6232205916600053, Discriminator Loss: [0.3152291 0.       ]\n","  Epoch: 305, Generator Loss: 2.609434232654342, Discriminator Loss: [0.3167221 0.       ]\n","  Epoch: 306, Generator Loss: 2.611275140061436, Discriminator Loss: [0.31136286 0.        ]\n","  Epoch: 307, Generator Loss: 2.6324145161961936, Discriminator Loss: [0.3151512 0.       ]\n","  Epoch: 308, Generator Loss: 2.6566512081996505, Discriminator Loss: [0.31821907 0.        ]\n","  Epoch: 309, Generator Loss: 2.648321515106293, Discriminator Loss: [0.3188855 0.       ]\n","  Epoch: 310, Generator Loss: 2.554427587842367, Discriminator Loss: [0.3168897 0.       ]\n","  Epoch: 311, Generator Loss: 2.5261244342987794, Discriminator Loss: [0.3174475 0.       ]\n","  Epoch: 312, Generator Loss: 2.581257660704923, Discriminator Loss: [0.31474236 0.        ]\n","  Epoch: 313, Generator Loss: 2.590823921812586, Discriminator Loss: [0.31758597 0.        ]\n","  Epoch: 314, Generator Loss: 2.647032319781292, Discriminator Loss: [0.31920087 0.        ]\n","  Epoch: 315, Generator Loss: 2.618219580995031, Discriminator Loss: [0.31660023 0.        ]\n","  Epoch: 316, Generator Loss: 2.522701669888324, Discriminator Loss: [0.31953043 0.        ]\n","  Epoch: 317, Generator Loss: 2.604439380657242, Discriminator Loss: [0.3166033 0.       ]\n","  Epoch: 318, Generator Loss: 2.560917681958302, Discriminator Loss: [0.3161483 0.       ]\n","  Epoch: 319, Generator Loss: 2.5668620761618555, Discriminator Loss: [0.31784573 0.        ]\n","  Epoch: 320, Generator Loss: 2.555061600294458, Discriminator Loss: [0.31262055 0.        ]\n","  Epoch: 321, Generator Loss: 2.5901462471628762, Discriminator Loss: [0.3185902 0.       ]\n","  Epoch: 322, Generator Loss: 2.6006482853946915, Discriminator Loss: [0.32113847 0.        ]\n","  Epoch: 323, Generator Loss: 2.670524730739823, Discriminator Loss: [0.31605422 0.        ]\n","  Epoch: 324, Generator Loss: 2.685581134026309, Discriminator Loss: [0.31816557 0.        ]\n","  Epoch: 325, Generator Loss: 2.6752975662070586, Discriminator Loss: [0.31800318 0.        ]\n","  Epoch: 326, Generator Loss: 2.623310883361173, Discriminator Loss: [0.31716076 0.        ]\n","  Epoch: 327, Generator Loss: 2.607219260859202, Discriminator Loss: [0.31350654 0.        ]\n","  Epoch: 328, Generator Loss: 2.5914876202502883, Discriminator Loss: [0.31468067 0.        ]\n","  Epoch: 329, Generator Loss: 2.613815579069666, Discriminator Loss: [0.31669122 0.        ]\n","  Epoch: 330, Generator Loss: 2.6134096484586418, Discriminator Loss: [0.31452993 0.        ]\n","  Epoch: 331, Generator Loss: 2.544870882149202, Discriminator Loss: [0.31580824 0.        ]\n","  Epoch: 332, Generator Loss: 2.6296489052025667, Discriminator Loss: [0.31303474 0.        ]\n","  Epoch: 333, Generator Loss: 2.578856462455658, Discriminator Loss: [0.31657547 0.        ]\n","  Epoch: 334, Generator Loss: 2.5950021786862107, Discriminator Loss: [0.30954084 0.        ]\n","  Epoch: 335, Generator Loss: 2.539686373917453, Discriminator Loss: [0.3162744 0.       ]\n","  Epoch: 336, Generator Loss: 2.5995630399290337, Discriminator Loss: [0.31496134 0.        ]\n","  Epoch: 337, Generator Loss: 2.5187755564609207, Discriminator Loss: [0.31747875 0.        ]\n","  Epoch: 338, Generator Loss: 2.5541485734732756, Discriminator Loss: [0.3128996 0.       ]\n","  Epoch: 339, Generator Loss: 2.5746522208294236, Discriminator Loss: [0.31393918 0.        ]\n","  Epoch: 340, Generator Loss: 2.5890161387891655, Discriminator Loss: [0.31618935 0.        ]\n","  Epoch: 341, Generator Loss: 2.642326469880989, Discriminator Loss: [0.3153966 0.       ]\n","  Epoch: 342, Generator Loss: 2.6061837630099562, Discriminator Loss: [0.31595734 0.        ]\n","  Epoch: 343, Generator Loss: 2.589789792715785, Discriminator Loss: [0.32063308 0.        ]\n","  Epoch: 344, Generator Loss: 2.5985211306307687, Discriminator Loss: [0.31471115 0.        ]\n","  Epoch: 345, Generator Loss: 2.6200593767395937, Discriminator Loss: [0.3150027 0.       ]\n","  Epoch: 346, Generator Loss: 2.5964745900717126, Discriminator Loss: [0.31742147 0.        ]\n","  Epoch: 347, Generator Loss: 2.661574001771858, Discriminator Loss: [0.31707418 0.        ]\n","  Epoch: 348, Generator Loss: 2.612135286790779, Discriminator Loss: [0.31534696 0.        ]\n","  Epoch: 349, Generator Loss: 2.654101674815258, Discriminator Loss: [0.31824598 0.        ]\n","  Epoch: 350, Generator Loss: 2.556895007570106, Discriminator Loss: [0.31436968 0.        ]\n","  Epoch: 351, Generator Loss: 2.6699733662318033, Discriminator Loss: [0.3169071 0.       ]\n","  Epoch: 352, Generator Loss: 2.6566541898681457, Discriminator Loss: [0.31651115 0.        ]\n","  Epoch: 353, Generator Loss: 2.5835897879428176, Discriminator Loss: [0.31258506 0.        ]\n","  Epoch: 354, Generator Loss: 2.672942836600614, Discriminator Loss: [0.31329796 0.        ]\n","  Epoch: 355, Generator Loss: 2.619711376098265, Discriminator Loss: [0.31363082 0.        ]\n","  Epoch: 356, Generator Loss: 2.5908633729061448, Discriminator Loss: [0.31397918 0.        ]\n","  Epoch: 357, Generator Loss: 2.669139438364879, Discriminator Loss: [0.31314644 0.        ]\n","  Epoch: 358, Generator Loss: 2.621165297117578, Discriminator Loss: [0.31395817 0.        ]\n","  Epoch: 359, Generator Loss: 2.6405578377735184, Discriminator Loss: [0.3150257 0.       ]\n","  Epoch: 360, Generator Loss: 2.607024870723127, Discriminator Loss: [0.3145664 0.       ]\n","  Epoch: 361, Generator Loss: 2.6410147210201584, Discriminator Loss: [0.31873783 0.        ]\n","  Epoch: 362, Generator Loss: 2.694138736609953, Discriminator Loss: [0.3148355 0.       ]\n","  Epoch: 363, Generator Loss: 2.6465771370623483, Discriminator Loss: [0.31765682 0.        ]\n","  Epoch: 364, Generator Loss: 2.6227513738425383, Discriminator Loss: [0.3159553 0.       ]\n","  Epoch: 365, Generator Loss: 2.5348443769546876, Discriminator Loss: [0.31314984 0.        ]\n","  Epoch: 366, Generator Loss: 2.590830018721431, Discriminator Loss: [0.31396806 0.        ]\n","  Epoch: 367, Generator Loss: 2.6107499556369094, Discriminator Loss: [0.31798676 0.        ]\n","  Epoch: 368, Generator Loss: 2.5936248647161277, Discriminator Loss: [0.31383055 0.        ]\n","  Epoch: 369, Generator Loss: 2.6574004084230904, Discriminator Loss: [0.31579342 0.        ]\n","  Epoch: 370, Generator Loss: 2.5831844935934227, Discriminator Loss: [0.3130854 0.       ]\n","  Epoch: 371, Generator Loss: 2.699966506785657, Discriminator Loss: [0.31395888 0.        ]\n","  Epoch: 372, Generator Loss: 2.6101282389767198, Discriminator Loss: [0.31659257 0.        ]\n","  Epoch: 373, Generator Loss: 2.5796057632170526, Discriminator Loss: [0.3159944 0.       ]\n","  Epoch: 374, Generator Loss: 2.6181456669267402, Discriminator Loss: [0.31306142 0.        ]\n","  Epoch: 375, Generator Loss: 2.7049979474171097, Discriminator Loss: [0.3152429 0.       ]\n","  Epoch: 376, Generator Loss: 2.5921535448855666, Discriminator Loss: [0.31384772 0.        ]\n","  Epoch: 377, Generator Loss: 2.6309025086552262, Discriminator Loss: [0.32034022 0.        ]\n","  Epoch: 378, Generator Loss: 2.647594989064228, Discriminator Loss: [0.3133394 0.       ]\n","  Epoch: 379, Generator Loss: 2.6662227820201094, Discriminator Loss: [0.31386378 0.        ]\n","  Epoch: 380, Generator Loss: 2.7056704142007484, Discriminator Loss: [0.31198487 0.        ]\n","  Epoch: 381, Generator Loss: 2.5102255732180123, Discriminator Loss: [0.3138118 0.       ]\n","  Epoch: 382, Generator Loss: 2.5102054699357734, Discriminator Loss: [0.30972797 0.        ]\n","  Epoch: 383, Generator Loss: 2.5588923376726815, Discriminator Loss: [0.31261206 0.        ]\n","  Epoch: 384, Generator Loss: 2.5936964779015046, Discriminator Loss: [0.31428075 0.        ]\n","  Epoch: 385, Generator Loss: 2.648414241262229, Discriminator Loss: [0.31112877 0.        ]\n","  Epoch: 386, Generator Loss: 2.665824859975332, Discriminator Loss: [0.31420174 0.        ]\n","  Epoch: 387, Generator Loss: 2.6052686449993088, Discriminator Loss: [0.31568408 0.        ]\n","  Epoch: 388, Generator Loss: 2.7054360659725694, Discriminator Loss: [0.3115744 0.       ]\n","  Epoch: 389, Generator Loss: 2.6228022604103547, Discriminator Loss: [0.31541115 0.        ]\n","  Epoch: 390, Generator Loss: 2.569818748048989, Discriminator Loss: [0.31374845 0.        ]\n","  Epoch: 391, Generator Loss: 2.5912974799971984, Discriminator Loss: [0.3094602 0.       ]\n","  Epoch: 392, Generator Loss: 2.70839163912348, Discriminator Loss: [0.31848532 0.        ]\n","  Epoch: 393, Generator Loss: 2.658602453139891, Discriminator Loss: [0.3132752 0.       ]\n","  Epoch: 394, Generator Loss: 2.665422498461712, Discriminator Loss: [0.31237838 0.        ]\n","  Epoch: 395, Generator Loss: 2.720872662153589, Discriminator Loss: [0.31623295 0.        ]\n","  Epoch: 396, Generator Loss: 2.584352728832199, Discriminator Loss: [0.3137823 0.       ]\n","  Epoch: 397, Generator Loss: 2.7600813802466333, Discriminator Loss: [0.3122784 0.       ]\n","  Epoch: 398, Generator Loss: 2.62430330213294, Discriminator Loss: [0.3136362 0.       ]\n","  Epoch: 399, Generator Loss: 2.5722933907106698, Discriminator Loss: [0.3123148 0.       ]\n","  Epoch: 400, Generator Loss: 2.629021159137588, Discriminator Loss: [0.31255266 0.        ]\n","  Epoch: 401, Generator Loss: 2.642222362828542, Discriminator Loss: [0.3132972 0.       ]\n","  Epoch: 402, Generator Loss: 2.6139397247728096, Discriminator Loss: [0.31118917 0.        ]\n","  Epoch: 403, Generator Loss: 2.6513060905847206, Discriminator Loss: [0.31393492 0.        ]\n","  Epoch: 404, Generator Loss: 2.6438868074532014, Discriminator Loss: [0.31470862 0.        ]\n","  Epoch: 405, Generator Loss: 2.6231314690716294, Discriminator Loss: [0.31202546 0.        ]\n","  Epoch: 406, Generator Loss: 2.6277214647775673, Discriminator Loss: [0.31480494 0.        ]\n","  Epoch: 407, Generator Loss: 2.7113654814570785, Discriminator Loss: [0.31474128 0.        ]\n","  Epoch: 408, Generator Loss: 2.7531708234764007, Discriminator Loss: [0.31297666 0.        ]\n","  Epoch: 409, Generator Loss: 2.6884098684931375, Discriminator Loss: [0.31632742 0.        ]\n","  Epoch: 410, Generator Loss: 2.577893176710749, Discriminator Loss: [0.3117007 0.       ]\n","  Epoch: 411, Generator Loss: 2.5826365531209006, Discriminator Loss: [0.3099321 0.       ]\n","  Epoch: 412, Generator Loss: 2.5768303411552704, Discriminator Loss: [0.30891645 0.        ]\n","  Epoch: 413, Generator Loss: 2.566435395953167, Discriminator Loss: [0.31459883 0.        ]\n","  Epoch: 414, Generator Loss: 2.613730960581676, Discriminator Loss: [0.31067538 0.        ]\n","  Epoch: 415, Generator Loss: 2.635042885699904, Discriminator Loss: [0.31190282 0.        ]\n","  Epoch: 416, Generator Loss: 2.6141435051538857, Discriminator Loss: [0.31194 0.     ]\n","  Epoch: 417, Generator Loss: 2.5459393650652413, Discriminator Loss: [0.30921188 0.        ]\n","  Epoch: 418, Generator Loss: 2.5611017669539855, Discriminator Loss: [0.30676508 0.        ]\n","  Epoch: 419, Generator Loss: 2.5770689478839737, Discriminator Loss: [0.31244412 0.        ]\n","  Epoch: 420, Generator Loss: 2.7227568770029458, Discriminator Loss: [0.31367397 0.        ]\n","  Epoch: 421, Generator Loss: 2.719760991004576, Discriminator Loss: [0.31499717 0.        ]\n","  Epoch: 422, Generator Loss: 2.643924010805337, Discriminator Loss: [0.3130851 0.       ]\n","  Epoch: 423, Generator Loss: 2.65921716804964, Discriminator Loss: [0.31587544 0.        ]\n","  Epoch: 424, Generator Loss: 2.67243579209569, Discriminator Loss: [0.31480816 0.        ]\n","  Epoch: 425, Generator Loss: 2.666930192924408, Discriminator Loss: [0.3211663 0.       ]\n","  Epoch: 426, Generator Loss: 2.7167876958847046, Discriminator Loss: [0.3141406 0.       ]\n","  Epoch: 427, Generator Loss: 2.579081559755716, Discriminator Loss: [0.3145703 0.       ]\n","  Epoch: 428, Generator Loss: 2.594706098717379, Discriminator Loss: [0.31490937 0.        ]\n","  Epoch: 429, Generator Loss: 2.6603082547704857, Discriminator Loss: [0.31055376 0.        ]\n","  Epoch: 430, Generator Loss: 2.6546825055616448, Discriminator Loss: [0.31240952 0.        ]\n","  Epoch: 431, Generator Loss: 2.5422164862414443, Discriminator Loss: [0.3102606 0.       ]\n","  Epoch: 432, Generator Loss: 2.633591807032206, Discriminator Loss: [0.30940905 0.        ]\n","  Epoch: 433, Generator Loss: 2.567452104694872, Discriminator Loss: [0.30789363 0.        ]\n","  Epoch: 434, Generator Loss: 2.6656979264983214, Discriminator Loss: [0.3114735 0.       ]\n","  Epoch: 435, Generator Loss: 2.6182175056043877, Discriminator Loss: [0.31326514 0.        ]\n","  Epoch: 436, Generator Loss: 2.6324494919144965, Discriminator Loss: [0.31317556 0.        ]\n","  Epoch: 437, Generator Loss: 2.6305518509393715, Discriminator Loss: [0.3119545 0.       ]\n","  Epoch: 438, Generator Loss: 2.6563842612576773, Discriminator Loss: [0.310041 0.      ]\n","  Epoch: 439, Generator Loss: 2.550551224903888, Discriminator Loss: [0.31178126 0.        ]\n","  Epoch: 440, Generator Loss: 2.5847048802548143, Discriminator Loss: [0.31283844 0.        ]\n","  Epoch: 441, Generator Loss: 2.6695256951343582, Discriminator Loss: [0.31578505 0.        ]\n","  Epoch: 442, Generator Loss: 2.7031809579895203, Discriminator Loss: [0.31181774 0.        ]\n","  Epoch: 443, Generator Loss: 2.675588708326041, Discriminator Loss: [0.31229857 0.        ]\n","  Epoch: 444, Generator Loss: 2.610616356493479, Discriminator Loss: [0.31243858 0.        ]\n","  Epoch: 445, Generator Loss: 2.6838266174477265, Discriminator Loss: [0.3133844 0.       ]\n","  Epoch: 446, Generator Loss: 2.6098053943679993, Discriminator Loss: [0.31196177 0.        ]\n","  Epoch: 447, Generator Loss: 2.5590679027948036, Discriminator Loss: [0.31178454 0.        ]\n","  Epoch: 448, Generator Loss: 2.655775456543428, Discriminator Loss: [0.31471884 0.        ]\n","  Epoch: 449, Generator Loss: 2.5728905028607474, Discriminator Loss: [0.31513235 0.        ]\n","  Epoch: 450, Generator Loss: 2.682097146310002, Discriminator Loss: [0.31622863 0.        ]\n","  Epoch: 451, Generator Loss: 2.634836956679103, Discriminator Loss: [0.31436074 0.        ]\n","  Epoch: 452, Generator Loss: 2.5607226067278757, Discriminator Loss: [0.3086934 0.       ]\n","  Epoch: 453, Generator Loss: 2.6094991284680655, Discriminator Loss: [0.30653092 0.        ]\n","  Epoch: 454, Generator Loss: 2.612330879073545, Discriminator Loss: [0.30913955 0.        ]\n","  Epoch: 455, Generator Loss: 2.6319524696074335, Discriminator Loss: [0.31140232 0.        ]\n","  Epoch: 456, Generator Loss: 2.6365886309060707, Discriminator Loss: [0.31156033 0.        ]\n","  Epoch: 457, Generator Loss: 2.7350313922008835, Discriminator Loss: [0.31221876 0.        ]\n","  Epoch: 458, Generator Loss: 2.6301903853933495, Discriminator Loss: [0.31054637 0.        ]\n","  Epoch: 459, Generator Loss: 2.7128007569945, Discriminator Loss: [0.31114134 0.        ]\n","  Epoch: 460, Generator Loss: 2.652694376118212, Discriminator Loss: [0.31390172 0.        ]\n","  Epoch: 461, Generator Loss: 2.733856028821095, Discriminator Loss: [0.31497478 0.        ]\n","  Epoch: 462, Generator Loss: 2.700823555509728, Discriminator Loss: [0.30985013 0.        ]\n","  Epoch: 463, Generator Loss: 2.607895382915635, Discriminator Loss: [0.31445116 0.        ]\n","  Epoch: 464, Generator Loss: 2.59403629188078, Discriminator Loss: [0.30983087 0.        ]\n","  Epoch: 465, Generator Loss: 2.587859208325306, Discriminator Loss: [0.3116118 0.       ]\n","  Epoch: 466, Generator Loss: 2.5525861978530884, Discriminator Loss: [0.30797872 0.        ]\n","  Epoch: 467, Generator Loss: 2.590752838605858, Discriminator Loss: [0.30765998 0.        ]\n","  Epoch: 468, Generator Loss: 2.5486520672418984, Discriminator Loss: [0.30928826 0.        ]\n","  Epoch: 469, Generator Loss: 2.571812138499984, Discriminator Loss: [0.30992946 0.        ]\n","  Epoch: 470, Generator Loss: 2.624350312244461, Discriminator Loss: [0.31351712 0.        ]\n","  Epoch: 471, Generator Loss: 2.5437364334083465, Discriminator Loss: [0.31006235 0.        ]\n","  Epoch: 472, Generator Loss: 2.6577287108065133, Discriminator Loss: [0.31064773 0.        ]\n","  Epoch: 473, Generator Loss: 2.6483177607318007, Discriminator Loss: [0.31353953 0.        ]\n","  Epoch: 474, Generator Loss: 2.8148689140756447, Discriminator Loss: [0.31093514 0.        ]\n","  Epoch: 475, Generator Loss: 2.637256991432374, Discriminator Loss: [0.31199804 0.        ]\n","  Epoch: 476, Generator Loss: 2.6466162420180908, Discriminator Loss: [0.3108574 0.       ]\n","  Epoch: 477, Generator Loss: 2.704915672899729, Discriminator Loss: [0.3102694 0.       ]\n","  Epoch: 478, Generator Loss: 2.644926048186888, Discriminator Loss: [0.31266716 0.        ]\n","  Epoch: 479, Generator Loss: 2.6256233267037263, Discriminator Loss: [0.311059 0.      ]\n","  Epoch: 480, Generator Loss: 2.691344723644027, Discriminator Loss: [0.30863276 0.        ]\n","  Epoch: 481, Generator Loss: 2.6245522858148598, Discriminator Loss: [0.3123596 0.       ]\n","  Epoch: 482, Generator Loss: 2.6995674500982445, Discriminator Loss: [0.31165856 0.        ]\n","  Epoch: 483, Generator Loss: 2.636158634381122, Discriminator Loss: [0.30994377 0.        ]\n","  Epoch: 484, Generator Loss: 2.7217935481703424, Discriminator Loss: [0.31134036 0.        ]\n","  Epoch: 485, Generator Loss: 2.5439578208578637, Discriminator Loss: [0.30957308 0.        ]\n","  Epoch: 486, Generator Loss: 2.61573340950242, Discriminator Loss: [0.30873153 0.        ]\n","  Epoch: 487, Generator Loss: 2.552468697708773, Discriminator Loss: [0.30845514 0.        ]\n","  Epoch: 488, Generator Loss: 2.5657285092824913, Discriminator Loss: [0.30847135 0.        ]\n","  Epoch: 489, Generator Loss: 2.6129961530846284, Discriminator Loss: [0.30639762 0.        ]\n","  Epoch: 490, Generator Loss: 2.629510121173169, Discriminator Loss: [0.30925044 0.        ]\n","  Epoch: 491, Generator Loss: 2.610347704715039, Discriminator Loss: [0.30846852 0.        ]\n","  Epoch: 492, Generator Loss: 2.6921386072434577, Discriminator Loss: [0.31168938 0.        ]\n","  Epoch: 493, Generator Loss: 2.5233229729066413, Discriminator Loss: [0.3113411 0.       ]\n","  Epoch: 494, Generator Loss: 2.5776259841689146, Discriminator Loss: [0.3101539 0.       ]\n","  Epoch: 495, Generator Loss: 2.630620489637536, Discriminator Loss: [0.30465144 0.        ]\n","  Epoch: 496, Generator Loss: 2.7328638700117547, Discriminator Loss: [0.30925512 0.        ]\n","  Epoch: 497, Generator Loss: 2.6728619811046554, Discriminator Loss: [0.31064838 0.        ]\n","  Epoch: 498, Generator Loss: 2.580509240368763, Discriminator Loss: [0.30851737 0.        ]\n","  Epoch: 499, Generator Loss: 2.5927186012268066, Discriminator Loss: [0.3105817 0.       ]\n","  Epoch: 500, Generator Loss: 2.694595257919955, Discriminator Loss: [0.30890757 0.        ]\n","  Epoch: 501, Generator Loss: 2.6694799101496316, Discriminator Loss: [0.31538478 0.        ]\n","  Epoch: 502, Generator Loss: 2.6613995195871376, Discriminator Loss: [0.31145856 0.        ]\n","  Epoch: 503, Generator Loss: 2.6534485730780175, Discriminator Loss: [0.31184205 0.        ]\n","  Epoch: 504, Generator Loss: 2.7320421244724686, Discriminator Loss: [0.30979133 0.        ]\n","  Epoch: 505, Generator Loss: 2.596787501530475, Discriminator Loss: [0.30941796 0.        ]\n","  Epoch: 506, Generator Loss: 2.5629851013781075, Discriminator Loss: [0.30873543 0.        ]\n","  Epoch: 507, Generator Loss: 2.6274848731167344, Discriminator Loss: [0.3092825 0.       ]\n","  Epoch: 508, Generator Loss: 2.524878150009247, Discriminator Loss: [0.3080082 0.       ]\n","  Epoch: 509, Generator Loss: 2.550277108169464, Discriminator Loss: [0.30604228 0.        ]\n","  Epoch: 510, Generator Loss: 2.6508206990827996, Discriminator Loss: [0.30978784 0.        ]\n","  Epoch: 511, Generator Loss: 2.7078449237777527, Discriminator Loss: [0.3115079 0.       ]\n","  Epoch: 512, Generator Loss: 2.590518892529499, Discriminator Loss: [0.31013882 0.        ]\n","  Epoch: 513, Generator Loss: 2.6120357312351823, Discriminator Loss: [0.30853343 0.        ]\n","  Epoch: 514, Generator Loss: 2.531523685857474, Discriminator Loss: [0.30829358 0.        ]\n","  Epoch: 515, Generator Loss: 2.610209894467549, Discriminator Loss: [0.30886906 0.        ]\n","  Epoch: 516, Generator Loss: 2.5842116094497314, Discriminator Loss: [0.30833924 0.        ]\n","  Epoch: 517, Generator Loss: 2.6058796112795912, Discriminator Loss: [0.3088739 0.       ]\n","  Epoch: 518, Generator Loss: 2.6362822041454086, Discriminator Loss: [0.31020632 0.        ]\n","  Epoch: 519, Generator Loss: 2.648969825491848, Discriminator Loss: [0.30792204 0.        ]\n","  Epoch: 520, Generator Loss: 2.592331846076322, Discriminator Loss: [0.31153923 0.        ]\n","  Epoch: 521, Generator Loss: 2.472694184406694, Discriminator Loss: [0.30741924 0.        ]\n","  Epoch: 522, Generator Loss: 2.51735382051353, Discriminator Loss: [0.30803072 0.        ]\n","  Epoch: 523, Generator Loss: 2.5420701819730094, Discriminator Loss: [0.30692545 0.        ]\n","  Epoch: 524, Generator Loss: 2.426720067679164, Discriminator Loss: [0.30702916 0.        ]\n","  Epoch: 525, Generator Loss: 2.588448597724179, Discriminator Loss: [0.31433734 0.        ]\n","  Epoch: 526, Generator Loss: 2.646238285374929, Discriminator Loss: [0.3190432 0.       ]\n","  Epoch: 527, Generator Loss: 2.5349701958966544, Discriminator Loss: [0.30888408 0.        ]\n","  Epoch: 528, Generator Loss: 2.6282745025244103, Discriminator Loss: [0.31135753 0.        ]\n","  Epoch: 529, Generator Loss: 2.5959028867353875, Discriminator Loss: [0.31033584 0.        ]\n","  Epoch: 530, Generator Loss: 2.551593685724649, Discriminator Loss: [0.30437735 0.        ]\n","  Epoch: 531, Generator Loss: 2.582163631197918, Discriminator Loss: [0.30617172 0.        ]\n","  Epoch: 532, Generator Loss: 2.5217681979558555, Discriminator Loss: [0.30814478 0.        ]\n","  Epoch: 533, Generator Loss: 2.651903794472476, Discriminator Loss: [0.31699303 0.        ]\n","  Epoch: 534, Generator Loss: 2.5880706382085044, Discriminator Loss: [0.3106444 0.       ]\n","  Epoch: 535, Generator Loss: 2.632746756794941, Discriminator Loss: [0.30681163 0.        ]\n","  Epoch: 536, Generator Loss: 2.648654772574643, Discriminator Loss: [0.31038758 0.        ]\n","  Epoch: 537, Generator Loss: 2.575091555894139, Discriminator Loss: [0.31020728 0.        ]\n","  Epoch: 538, Generator Loss: 2.584242737436869, Discriminator Loss: [0.30858603 0.        ]\n","  Epoch: 539, Generator Loss: 2.4617483012647514, Discriminator Loss: [0.30613065 0.        ]\n","  Epoch: 540, Generator Loss: 2.613854023347418, Discriminator Loss: [0.30748126 0.        ]\n","  Epoch: 541, Generator Loss: 2.5205518050366136, Discriminator Loss: [0.30334574 0.        ]\n","  Epoch: 542, Generator Loss: 2.6029774079839867, Discriminator Loss: [0.3075216 0.       ]\n","  Epoch: 543, Generator Loss: 2.6227797499622207, Discriminator Loss: [0.31033048 0.        ]\n","  Epoch: 544, Generator Loss: 2.629033602863909, Discriminator Loss: [0.30877098 0.        ]\n","  Epoch: 545, Generator Loss: 2.5658602140035973, Discriminator Loss: [0.30783486 0.        ]\n","  Epoch: 546, Generator Loss: 2.5714529896356972, Discriminator Loss: [0.306357 0.      ]\n","  Epoch: 547, Generator Loss: 2.6225526390305482, Discriminator Loss: [0.30861342 0.        ]\n","  Epoch: 548, Generator Loss: 2.52959017293999, Discriminator Loss: [0.30780932 0.        ]\n","  Epoch: 549, Generator Loss: 2.670272394835231, Discriminator Loss: [0.31278414 0.        ]\n","  Epoch: 550, Generator Loss: 2.5500311822776336, Discriminator Loss: [0.30943576 0.        ]\n","  Epoch: 551, Generator Loss: 2.699561096099486, Discriminator Loss: [0.3111982 0.       ]\n","  Epoch: 552, Generator Loss: 2.600665096777031, Discriminator Loss: [0.3105727 0.       ]\n","  Epoch: 553, Generator Loss: 2.650342503225947, Discriminator Loss: [0.3135158 0.       ]\n","  Epoch: 554, Generator Loss: 2.5367899271379035, Discriminator Loss: [0.31089857 0.        ]\n","  Epoch: 555, Generator Loss: 2.507146693137755, Discriminator Loss: [0.30608407 0.        ]\n","  Epoch: 556, Generator Loss: 2.4937534203012306, Discriminator Loss: [0.3103821 0.       ]\n","  Epoch: 557, Generator Loss: 2.472047057496496, Discriminator Loss: [0.3113847 0.       ]\n","  Epoch: 558, Generator Loss: 2.5456229275967703, Discriminator Loss: [0.307982 0.      ]\n","  Epoch: 559, Generator Loss: 2.641310036900532, Discriminator Loss: [0.30553973 0.        ]\n","  Epoch: 560, Generator Loss: 2.639700205929308, Discriminator Loss: [0.30578268 0.        ]\n","  Epoch: 561, Generator Loss: 2.6534812938736145, Discriminator Loss: [0.3092557 0.       ]\n","  Epoch: 562, Generator Loss: 2.5904646953904487, Discriminator Loss: [0.31240815 0.        ]\n","  Epoch: 563, Generator Loss: 2.6260693159448096, Discriminator Loss: [0.3088122 0.       ]\n","  Epoch: 564, Generator Loss: 2.6965103795729486, Discriminator Loss: [0.3096815 0.       ]\n","  Epoch: 565, Generator Loss: 2.6654657556349974, Discriminator Loss: [0.31400168 0.        ]\n","  Epoch: 566, Generator Loss: 2.605734184563878, Discriminator Loss: [0.31010312 0.        ]\n","  Epoch: 567, Generator Loss: 2.679001721991114, Discriminator Loss: [0.3141683 0.       ]\n","  Epoch: 568, Generator Loss: 2.6554191773196303, Discriminator Loss: [0.309104 0.      ]\n","  Epoch: 569, Generator Loss: 2.57501134096858, Discriminator Loss: [0.30782017 0.        ]\n","  Epoch: 570, Generator Loss: 2.5248225640101607, Discriminator Loss: [0.3083877 0.       ]\n","  Epoch: 571, Generator Loss: 2.5309204193482917, Discriminator Loss: [0.3120249 0.       ]\n","  Epoch: 572, Generator Loss: 2.600776043282934, Discriminator Loss: [0.30762637 0.        ]\n","  Epoch: 573, Generator Loss: 2.565197686114943, Discriminator Loss: [0.30792832 0.        ]\n","  Epoch: 574, Generator Loss: 2.5233572721481323, Discriminator Loss: [0.30880854 0.        ]\n","  Epoch: 575, Generator Loss: 2.628642166953489, Discriminator Loss: [0.31178293 0.        ]\n","  Epoch: 576, Generator Loss: 2.4929085969924927, Discriminator Loss: [0.30179077 0.        ]\n","  Epoch: 577, Generator Loss: 2.5833211835608423, Discriminator Loss: [0.3065368 0.       ]\n","  Epoch: 578, Generator Loss: 2.589402172938887, Discriminator Loss: [0.30861142 0.        ]\n","  Epoch: 579, Generator Loss: 2.686926067593586, Discriminator Loss: [0.31263483 0.        ]\n","  Epoch: 580, Generator Loss: 2.649501125496554, Discriminator Loss: [0.30775222 0.        ]\n","  Epoch: 581, Generator Loss: 2.635429300457598, Discriminator Loss: [0.3062029 0.       ]\n","  Epoch: 582, Generator Loss: 2.6165783405303955, Discriminator Loss: [0.3108016 0.       ]\n","  Epoch: 583, Generator Loss: 2.6850089179464134, Discriminator Loss: [0.31018612 0.        ]\n","  Epoch: 584, Generator Loss: 2.587172605905188, Discriminator Loss: [0.30819777 0.        ]\n","  Epoch: 585, Generator Loss: 2.5433184396789734, Discriminator Loss: [0.30752394 0.        ]\n","  Epoch: 586, Generator Loss: 2.5798829506678755, Discriminator Loss: [0.31007823 0.        ]\n","  Epoch: 587, Generator Loss: 2.607146827571363, Discriminator Loss: [0.3069032 0.       ]\n","  Epoch: 588, Generator Loss: 2.6246020492300928, Discriminator Loss: [0.31016195 0.        ]\n","  Epoch: 589, Generator Loss: 2.6056569872132265, Discriminator Loss: [0.3060604 0.       ]\n","  Epoch: 590, Generator Loss: 2.6477045294750168, Discriminator Loss: [0.31159517 0.        ]\n","  Epoch: 591, Generator Loss: 2.559031779507557, Discriminator Loss: [0.3091686 0.       ]\n","  Epoch: 592, Generator Loss: 2.583732794566327, Discriminator Loss: [0.3036698 0.       ]\n","  Epoch: 593, Generator Loss: 2.5483119904276834, Discriminator Loss: [0.30705768 0.        ]\n","  Epoch: 594, Generator Loss: 2.600601401673742, Discriminator Loss: [0.3103347 0.       ]\n","  Epoch: 595, Generator Loss: 2.5145700925804046, Discriminator Loss: [0.30453432 0.        ]\n","  Epoch: 596, Generator Loss: 2.575209896248507, Discriminator Loss: [0.30398583 0.        ]\n","  Epoch: 597, Generator Loss: 2.527304209858538, Discriminator Loss: [0.3023572 0.       ]\n","  Epoch: 598, Generator Loss: 2.510445871985102, Discriminator Loss: [0.3040138 0.       ]\n","  Epoch: 599, Generator Loss: 2.579398360597082, Discriminator Loss: [0.30960482 0.        ]\n","  Epoch: 600, Generator Loss: 2.4798911493944833, Discriminator Loss: [0.30995136 0.        ]\n","  Epoch: 601, Generator Loss: 2.6696261684578584, Discriminator Loss: [0.31352168 0.        ]\n","  Epoch: 602, Generator Loss: 2.610104539308203, Discriminator Loss: [0.30547732 0.        ]\n","  Epoch: 603, Generator Loss: 2.5600359138235986, Discriminator Loss: [0.30834684 0.        ]\n","  Epoch: 604, Generator Loss: 2.587209425776838, Discriminator Loss: [0.30546874 0.        ]\n","  Epoch: 605, Generator Loss: 2.569086907857872, Discriminator Loss: [0.3045185 0.       ]\n","  Epoch: 606, Generator Loss: 2.4862299468143876, Discriminator Loss: [0.30643257 0.        ]\n","  Epoch: 607, Generator Loss: 2.633425557469747, Discriminator Loss: [0.31021658 0.        ]\n","  Epoch: 608, Generator Loss: 2.6046909682721977, Discriminator Loss: [0.30930153 0.        ]\n","  Epoch: 609, Generator Loss: 2.6102680212043854, Discriminator Loss: [0.30598977 0.        ]\n","  Epoch: 610, Generator Loss: 2.5871698583465026, Discriminator Loss: [0.30863172 0.        ]\n","  Epoch: 611, Generator Loss: 2.5505233227488504, Discriminator Loss: [0.30612946 0.        ]\n","  Epoch: 612, Generator Loss: 2.5468799062522063, Discriminator Loss: [0.3080813 0.       ]\n","  Epoch: 613, Generator Loss: 2.5508643374385604, Discriminator Loss: [0.30887568 0.        ]\n","  Epoch: 614, Generator Loss: 2.6283481968454567, Discriminator Loss: [0.31004462 0.        ]\n","  Epoch: 615, Generator Loss: 2.6982019579554177, Discriminator Loss: [0.30749798 0.        ]\n","  Epoch: 616, Generator Loss: 2.7208650586116745, Discriminator Loss: [0.31155476 0.        ]\n","  Epoch: 617, Generator Loss: 2.5486296199890504, Discriminator Loss: [0.3109658 0.       ]\n","  Epoch: 618, Generator Loss: 2.607602719801018, Discriminator Loss: [0.3077664 0.       ]\n","  Epoch: 619, Generator Loss: 2.5843237451760164, Discriminator Loss: [0.3059822 0.       ]\n","  Epoch: 620, Generator Loss: 2.4513145627745665, Discriminator Loss: [0.30687454 0.        ]\n","  Epoch: 621, Generator Loss: 2.5066135504159583, Discriminator Loss: [0.30605704 0.        ]\n","  Epoch: 622, Generator Loss: 2.458210788577436, Discriminator Loss: [0.3069272 0.       ]\n","  Epoch: 623, Generator Loss: 2.6375586139150413, Discriminator Loss: [0.31000277 0.        ]\n","  Epoch: 624, Generator Loss: 2.5300712226385094, Discriminator Loss: [0.30871323 0.        ]\n","  Epoch: 625, Generator Loss: 2.602997752557318, Discriminator Loss: [0.3085989 0.       ]\n","  Epoch: 626, Generator Loss: 2.6202271846403558, Discriminator Loss: [0.30898386 0.        ]\n","  Epoch: 627, Generator Loss: 2.6021148038197714, Discriminator Loss: [0.30747393 0.        ]\n","  Epoch: 628, Generator Loss: 2.664375952927463, Discriminator Loss: [0.30734527 0.        ]\n","  Epoch: 629, Generator Loss: 2.5425711252603187, Discriminator Loss: [0.30541283 0.        ]\n","  Epoch: 630, Generator Loss: 2.591164715318795, Discriminator Loss: [0.3078085 0.       ]\n","  Epoch: 631, Generator Loss: 2.5431228775575936, Discriminator Loss: [0.3047986 0.       ]\n","  Epoch: 632, Generator Loss: 2.1685152125645835, Discriminator Loss: [0.31080464 0.        ]\n","  Epoch: 633, Generator Loss: 2.3929029257900742, Discriminator Loss: [0.30593196 0.        ]\n","  Epoch: 634, Generator Loss: 2.4655610308589706, Discriminator Loss: [0.3018901 0.       ]\n","  Epoch: 635, Generator Loss: 2.3790722108749023, Discriminator Loss: [0.30463436 0.        ]\n","  Epoch: 636, Generator Loss: 2.42810937726354, Discriminator Loss: [0.3007864 0.       ]\n","  Epoch: 637, Generator Loss: 2.486864901450743, Discriminator Loss: [0.29981065 0.        ]\n","  Epoch: 638, Generator Loss: 2.408789873123169, Discriminator Loss: [0.30052406 0.        ]\n","  Epoch: 639, Generator Loss: 2.431433829916529, Discriminator Loss: [0.29807484 0.        ]\n","  Epoch: 640, Generator Loss: 2.4120796071477684, Discriminator Loss: [0.29866746 0.        ]\n","  Epoch: 641, Generator Loss: 2.4325156542192024, Discriminator Loss: [0.2983476 0.       ]\n","  Epoch: 642, Generator Loss: 2.4432236444519226, Discriminator Loss: [0.29756474 0.        ]\n","  Epoch: 643, Generator Loss: 2.4662799145801957, Discriminator Loss: [0.2984951 0.       ]\n","  Epoch: 644, Generator Loss: 2.382575672793101, Discriminator Loss: [2.9775345e-01 1.2048193e-04]\n","  Epoch: 645, Generator Loss: 2.413748686572155, Discriminator Loss: [0.2975869 0.       ]\n","  Epoch: 646, Generator Loss: 2.398484491440187, Discriminator Loss: [0.29998672 0.        ]\n","  Epoch: 647, Generator Loss: 2.4027514084275947, Discriminator Loss: [0.29755145 0.        ]\n","  Epoch: 648, Generator Loss: 2.4068217995655106, Discriminator Loss: [0.30275086 0.        ]\n","  Epoch: 649, Generator Loss: 2.5828474077833703, Discriminator Loss: [0.31824428 0.        ]\n","  Epoch: 650, Generator Loss: 2.467871055545577, Discriminator Loss: [0.30565217 0.        ]\n","  Epoch: 651, Generator Loss: 2.621407266122749, Discriminator Loss: [0.31271914 0.        ]\n","  Epoch: 652, Generator Loss: 2.602272195988391, Discriminator Loss: [0.30692023 0.        ]\n","  Epoch: 653, Generator Loss: 2.6212945702564285, Discriminator Loss: [0.31119436 0.        ]\n","  Epoch: 654, Generator Loss: 2.62982834821724, Discriminator Loss: [0.31060714 0.        ]\n","  Epoch: 655, Generator Loss: 2.658598533595901, Discriminator Loss: [0.31120715 0.        ]\n","  Epoch: 656, Generator Loss: 2.6290244381111787, Discriminator Loss: [0.311066 0.      ]\n","  Epoch: 657, Generator Loss: 2.5429063389100226, Discriminator Loss: [0.30621383 0.        ]\n","  Epoch: 658, Generator Loss: 2.6046244423073457, Discriminator Loss: [0.3079155 0.       ]\n","  Epoch: 659, Generator Loss: 2.6302250968404564, Discriminator Loss: [0.30774236 0.        ]\n","  Epoch: 660, Generator Loss: 2.558080611458744, Discriminator Loss: [0.30680972 0.        ]\n","  Epoch: 661, Generator Loss: 2.4778409636164285, Discriminator Loss: [0.30149004 0.        ]\n","  Epoch: 662, Generator Loss: 2.6437452428312187, Discriminator Loss: [0.30839276 0.        ]\n","  Epoch: 663, Generator Loss: 2.6034159660339355, Discriminator Loss: [0.3075309 0.       ]\n","  Epoch: 664, Generator Loss: 2.6164456261209694, Discriminator Loss: [0.30660033 0.        ]\n","  Epoch: 665, Generator Loss: 2.603442778070289, Discriminator Loss: [0.30502152 0.        ]\n","  Epoch: 666, Generator Loss: 2.672618134912238, Discriminator Loss: [0.3260664 0.       ]\n","  Epoch: 667, Generator Loss: 2.5885110705731864, Discriminator Loss: [0.30708867 0.        ]\n","  Epoch: 668, Generator Loss: 2.606065075081515, Discriminator Loss: [0.30748048 0.        ]\n","  Epoch: 669, Generator Loss: 2.5151681110083337, Discriminator Loss: [0.30655804 0.        ]\n","  Epoch: 670, Generator Loss: 2.4981870651245117, Discriminator Loss: [0.3031764 0.       ]\n","  Epoch: 671, Generator Loss: 2.4784983367805022, Discriminator Loss: [0.30691054 0.        ]\n","  Epoch: 672, Generator Loss: 2.538122270480696, Discriminator Loss: [0.3077734 0.       ]\n","  Epoch: 673, Generator Loss: 2.5595272722014464, Discriminator Loss: [0.3058943 0.       ]\n","  Epoch: 674, Generator Loss: 2.557800109127918, Discriminator Loss: [0.30517268 0.        ]\n","  Epoch: 675, Generator Loss: 2.5756733029721732, Discriminator Loss: [0.30287167 0.        ]\n","  Epoch: 676, Generator Loss: 2.506463753171714, Discriminator Loss: [0.30289754 0.        ]\n","  Epoch: 677, Generator Loss: 2.489785319351288, Discriminator Loss: [0.30083287 0.        ]\n","  Epoch: 678, Generator Loss: 2.498204699481826, Discriminator Loss: [0.30191174 0.        ]\n","  Epoch: 679, Generator Loss: 2.443336577300566, Discriminator Loss: [0.30396456 0.        ]\n","  Epoch: 680, Generator Loss: 2.539464189345578, Discriminator Loss: [0.30602992 0.        ]\n","  Epoch: 681, Generator Loss: 2.5003924082560713, Discriminator Loss: [0.30125275 0.        ]\n","  Epoch: 682, Generator Loss: 2.5928190874766153, Discriminator Loss: [0.30745196 0.        ]\n","  Epoch: 683, Generator Loss: 2.6776679452643335, Discriminator Loss: [0.30835873 0.        ]\n","  Epoch: 684, Generator Loss: 2.626212394381144, Discriminator Loss: [0.30627498 0.        ]\n","  Epoch: 685, Generator Loss: 2.556641746716327, Discriminator Loss: [0.30509844 0.        ]\n","  Epoch: 686, Generator Loss: 2.581123030329325, Discriminator Loss: [0.30441812 0.        ]\n","  Epoch: 687, Generator Loss: 2.650959603757743, Discriminator Loss: [0.3072045 0.       ]\n","  Epoch: 688, Generator Loss: 2.606090149247503, Discriminator Loss: [0.30380145 0.        ]\n","  Epoch: 689, Generator Loss: 2.5317224508308502, Discriminator Loss: [0.30446434 0.        ]\n","  Epoch: 690, Generator Loss: 2.5939920020390708, Discriminator Loss: [0.30710626 0.        ]\n","  Epoch: 691, Generator Loss: 2.5761734261570206, Discriminator Loss: [0.30437058 0.        ]\n","  Epoch: 692, Generator Loss: 2.525034957621471, Discriminator Loss: [0.30285907 0.        ]\n","  Epoch: 693, Generator Loss: 2.591674612229129, Discriminator Loss: [0.30256987 0.        ]\n","  Epoch: 694, Generator Loss: 2.5344363364828637, Discriminator Loss: [0.30071887 0.        ]\n","  Epoch: 695, Generator Loss: 2.4639048360916505, Discriminator Loss: [0.30247998 0.        ]\n","  Epoch: 696, Generator Loss: 2.458541100283703, Discriminator Loss: [0.30437464 0.        ]\n","  Epoch: 697, Generator Loss: 2.483973276184266, Discriminator Loss: [0.30181515 0.        ]\n","  Epoch: 698, Generator Loss: 2.516599775796913, Discriminator Loss: [0.30372712 0.        ]\n","  Epoch: 699, Generator Loss: 2.4572106657257997, Discriminator Loss: [0.30283168 0.        ]\n","  Epoch: 700, Generator Loss: 2.505019320062844, Discriminator Loss: [0.30191642 0.        ]\n","  Epoch: 701, Generator Loss: 2.545967709587281, Discriminator Loss: [0.30477205 0.        ]\n","  Epoch: 702, Generator Loss: 2.549839852804161, Discriminator Loss: [0.30134237 0.        ]\n","  Epoch: 703, Generator Loss: 2.473085426422487, Discriminator Loss: [0.30374813 0.        ]\n","  Epoch: 704, Generator Loss: 2.4579945328723953, Discriminator Loss: [0.30166304 0.        ]\n","  Epoch: 705, Generator Loss: 2.6002533665622574, Discriminator Loss: [0.30455115 0.        ]\n","  Epoch: 706, Generator Loss: 2.602927620152393, Discriminator Loss: [0.30510756 0.        ]\n","  Epoch: 707, Generator Loss: 2.576676233705268, Discriminator Loss: [0.30557898 0.        ]\n","  Epoch: 708, Generator Loss: 2.5271259417016823, Discriminator Loss: [0.30246338 0.        ]\n","  Epoch: 709, Generator Loss: 2.524139478982213, Discriminator Loss: [0.30213282 0.        ]\n","  Epoch: 710, Generator Loss: 2.647646892501647, Discriminator Loss: [0.30654088 0.        ]\n","  Epoch: 711, Generator Loss: 2.5318431251020317, Discriminator Loss: [0.3037168 0.       ]\n","  Epoch: 712, Generator Loss: 2.551459117108081, Discriminator Loss: [0.30297732 0.        ]\n","  Epoch: 713, Generator Loss: 2.5760411325707495, Discriminator Loss: [0.30162475 0.        ]\n","  Epoch: 714, Generator Loss: 2.572444237858416, Discriminator Loss: [0.30305326 0.        ]\n","  Epoch: 715, Generator Loss: 2.482987400997116, Discriminator Loss: [0.3039245 0.       ]\n","  Epoch: 716, Generator Loss: 2.532198210796678, Discriminator Loss: [0.30023208 0.        ]\n","  Epoch: 717, Generator Loss: 2.5878642582031617, Discriminator Loss: [0.30249125 0.        ]\n","  Epoch: 718, Generator Loss: 2.5891532093645577, Discriminator Loss: [0.30658844 0.        ]\n","  Epoch: 719, Generator Loss: 2.5424967975501556, Discriminator Loss: [0.30565017 0.        ]\n","  Epoch: 720, Generator Loss: 2.555210590362549, Discriminator Loss: [0.30637848 0.        ]\n","  Epoch: 721, Generator Loss: 2.5824696328266556, Discriminator Loss: [0.30258155 0.        ]\n","  Epoch: 722, Generator Loss: 2.5353411780782493, Discriminator Loss: [0.30299538 0.        ]\n","  Epoch: 723, Generator Loss: 2.564999660813665, Discriminator Loss: [0.30300206 0.        ]\n","  Epoch: 724, Generator Loss: 2.565125633435077, Discriminator Loss: [0.30204245 0.        ]\n","  Epoch: 725, Generator Loss: 2.532799602991127, Discriminator Loss: [0.303374 0.      ]\n","  Epoch: 726, Generator Loss: 2.584710794759084, Discriminator Loss: [0.30437827 0.        ]\n","  Epoch: 727, Generator Loss: 2.5548784330666785, Discriminator Loss: [0.30284196 0.        ]\n","  Epoch: 728, Generator Loss: 2.5805091197232164, Discriminator Loss: [0.30131698 0.        ]\n","  Epoch: 729, Generator Loss: 2.5038538840879876, Discriminator Loss: [0.3014671 0.       ]\n","  Epoch: 730, Generator Loss: 2.539415648184627, Discriminator Loss: [0.30204841 0.        ]\n","  Epoch: 731, Generator Loss: 2.6333315358104477, Discriminator Loss: [0.30195883 0.        ]\n","  Epoch: 732, Generator Loss: 2.5299935111080307, Discriminator Loss: [0.3019542 0.       ]\n","  Epoch: 733, Generator Loss: 2.510691626962409, Discriminator Loss: [0.302984 0.      ]\n","  Epoch: 734, Generator Loss: 2.3420900522944437, Discriminator Loss: [0.29968277 0.        ]\n","  Epoch: 735, Generator Loss: 2.399136988513441, Discriminator Loss: [0.30229026 0.        ]\n","  Epoch: 736, Generator Loss: 2.431540279503328, Discriminator Loss: [0.29815525 0.        ]\n","  Epoch: 737, Generator Loss: 2.4449765136442987, Discriminator Loss: [0.2989966 0.       ]\n","  Epoch: 738, Generator Loss: 2.3858278975429306, Discriminator Loss: [0.2962333 0.       ]\n","  Epoch: 739, Generator Loss: 2.4388153983885985, Discriminator Loss: [0.29606614 0.        ]\n","  Epoch: 740, Generator Loss: 2.3863591688225068, Discriminator Loss: [0.29996184 0.        ]\n","  Epoch: 741, Generator Loss: 2.40512567830373, Discriminator Loss: [0.29812327 0.        ]\n","  Epoch: 742, Generator Loss: 2.4032862243882143, Discriminator Loss: [0.29836163 0.        ]\n","  Epoch: 743, Generator Loss: 2.4029128235506723, Discriminator Loss: [0.2976393 0.       ]\n","  Epoch: 744, Generator Loss: 2.3929665720606423, Discriminator Loss: [0.29586148 0.        ]\n","  Epoch: 745, Generator Loss: 2.455735709293779, Discriminator Loss: [0.29629976 0.        ]\n"],"name":"stdout"}]},{"metadata":{"id":"yLcW00AWLSlv","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}